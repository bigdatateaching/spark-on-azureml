{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Spark Job on AzureML\n",
    "\n",
    "This notebook provides an example of how to define and run a job on AzureML using Spark. This notebook is the _control plane_, meaning it creates a connection to the AzureML workspace, defines the job, and submits the job.\n",
    "\n",
    "**This Jupyter notebook should be run from within a compute instance on AzureML, in a Python kernel, specifically `Python 3.10 - SDK v2 (Python 3.10.11)`**. \n",
    "\n",
    "As you can see from the files contained in this `job` subdirectory, there are several files:\n",
    "\n",
    "- A parametrized Python script with pyspark code that is submitted to a Spark cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a client connection to the AzureML workspace\n",
    "\n",
    "The following cell creates a connection object called `azureml_client` which has a connection to the AzureML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient, spark, Input, Output\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import UserIdentityConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this when running the control plane from the AzureML Compute Instance\n",
    "\n",
    "azureml_client = MLClient.from_config(\n",
    "    DefaultAzureCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1731375810369
    }
   },
   "outputs": [],
   "source": [
    "## Use this when running the control plane from your laptop\n",
    "ml_client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    workspace_name=\"prof-azureml\",\n",
    "    subscription_id=\"21ff0fc0-dd2c-450d-93b7-96eeb3699b22\",\n",
    "    resource_group_name=\"prof-azureml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Job\n",
    "\n",
    "The following cell defines the job. It is an object of [Spark Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.spark?view=azure-python) that contains the required information to run a job:\n",
    "\n",
    "- The cluster size\n",
    "- The script to run\n",
    "- The parameters for the script\n",
    "\n",
    "In the example below, we are using the `pyspark-script-job.py` script which is parametrized. As you can see, the parameters are the following:\n",
    "\n",
    "- `input_object_store_base_url` (**don't forget the trailing slashes /**): \n",
    "    - Here you will use a base URL of the `s3://<BUCKETNAME>/` form for Sagemaker, \n",
    "    - or `wasbs://<CONTAINER-NAME>@<STORAGE-ACCOUNT>.blob.core.windows.net/` \n",
    "    - or `azureml://datastores/workspaceblobstore/paths/` for AzureML. **Don't forget the trailing slash /.**\n",
    "- `input_path`: The path to read from\n",
    "- `output_object_store_base_url`: \n",
    "- `output_path`: The path to write to\n",
    "- `subreddits`: a comma separated string of subreddit names\n",
    "\n",
    "The PySpark script accepts the object store location for the raw data, in this case a single month. Then the job filters the original data and writes the filtered data out. This is designed to be used for either submissions or comments, not both.\n",
    "\n",
    "For more information about the parameters used in the job definition, [read the documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-submit-spark-jobs?view=azureml-api-2&tabs=sdk#submit-a-standalone-spark-job).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spark-nlp jar and save it locally. This needs to be done before submitting a job.\n",
    "import requests\n",
    "response = requests.get(\"https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-5.5.1.jar\")\n",
    "with open(\"spark-nlp-assembly-5.5.1.jar\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1731375844443
    }
   },
   "outputs": [],
   "source": [
    "nlp_job_def = spark(\n",
    "    display_name=\"test-sparknlp-job\",\n",
    "    code=\"./\",\n",
    "    entry={\"file\": \"test-job-sparknlp.py\"},\n",
    "    driver_cores=1,\n",
    "    driver_memory=\"7g\",\n",
    "    executor_cores=4,\n",
    "    executor_memory=\"7g\",\n",
    "    executor_instances=1,\n",
    "    resources={\n",
    "        \"instance_type\": \"Standard_E4S_V3\",\n",
    "        \"runtime_version\": \"3.4\",\n",
    "    },\n",
    "    environment=\"azureml://environments/sparknlp-env/versions/2\",\n",
    "    identity=UserIdentityConfiguration()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_job_def = spark(\n",
    "    display_name=\"test-sparknlp-job\",\n",
    "    code=\"./\",\n",
    "    entry={\"file\": \"test-job-sparknlp.py\"},\n",
    "    driver_cores=1,\n",
    "    driver_memory=\"7g\",\n",
    "    executor_cores=4,\n",
    "    executor_memory=\"7g\",\n",
    "    executor_instances=1,\n",
    "    resources={\n",
    "        \"instance_type\": \"Standard_E4S_V3\",\n",
    "        \"runtime_version\": \"3.4\",\n",
    "    },\n",
    "    environment=\"azureml:sparknlp-env@latest\",\n",
    "    identity=UserIdentityConfiguration()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job\n",
    "\n",
    "The following cell takes the job you defined above and submits it. If you are submitting multiple jobs, you may want to create separate job definition objects for clarity. You can submit more than one job, just remember that each job will spin up a Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1731375861539
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading azure-project-tutorial (0.56 MBs): 100%|██████████| 563469/563469 [00:00<00:00, 1164863.01it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_job_base_obj = ml_client.jobs.create_or_update(test_job_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlException",
     "evalue": "\n\u001b[37m\n\u001b[30m\n1) One or more fields are invalid\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Could not parse azureml://sparknlp-env@latest. If providing an ARM id, it should start with a '/'.\u001b[39m\n\nResolutions: \n1) Double-check that all specified parameters are of the correct types and formats prescribed by the ArmResource schema.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://docs.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:664\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# Create all dependent resources\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_or_upload_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationException, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m ex:  \u001b[38;5;66;03m# pylint: disable=W0718\u001b[39;00m\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:1086\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_upload_dependencies\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This method converts name or name:version to ARM id. Or it\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;124;03mregisters/uploads nested dependencies.\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m:rtype: Job\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1086\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_or_azureml_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orchestrators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_asset_arm_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, PipelineJob):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# Resolve top-level inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:1369\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_azureml_id\u001b[0;34m(self, job, resolver)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, Spark):\n\u001b[0;32m-> 1369\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_for_spark_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, ParallelJob):\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:1447\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_for_spark_job\u001b[0;34m(self, job, resolver)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     job\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m=\u001b[39m resolver(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m         Code(base_path\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39m_base_path, path\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mcode),\n\u001b[1;32m   1445\u001b[0m         azureml_type\u001b[38;5;241m=\u001b[39mAzureMLResourceType\u001b[38;5;241m.\u001b[39mCODE,\n\u001b[1;32m   1446\u001b[0m     )\n\u001b[0;32m-> 1447\u001b[0m job\u001b[38;5;241m.\u001b[39menvironment \u001b[38;5;241m=\u001b[39m \u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mazureml_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAzureMLResourceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mENVIRONMENT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m job\u001b[38;5;241m.\u001b[39mcompute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_compute_id(resolver, job\u001b[38;5;241m.\u001b[39mcompute)\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_operation_orchestrator.py:171\u001b[0m, in \u001b[0;36mOperationOrchestrator.get_asset_arm_id\u001b[0;34m(self, asset, azureml_type, register_asset, sub_workspace_resource)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mazureml_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_asset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m name, label \u001b[38;5;241m=\u001b[39m \u001b[43mparse_name_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43masset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# TODO: remove this condition after label is fully supported for all versioned resources\u001b[39;00m\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/_utils/_arm_id_utils.py:335\u001b[0m, in \u001b[0;36mparse_name_label\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m (name\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If providing an ARM id, it should start with a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    337\u001b[0m         no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m         error_type\u001b[38;5;241m=\u001b[39mValidationErrorType\u001b[38;5;241m.\u001b[39mINVALID_VALUE,\n\u001b[1;32m    339\u001b[0m         error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    340\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mARM_RESOURCE,\n\u001b[1;32m    341\u001b[0m     )\n\u001b[1;32m    342\u001b[0m token_list \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValidationException\u001b[0m: Could not parse azureml://sparknlp-env@latest. If providing an ARM id, it should start with a '/'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sparknlp_job \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp_job_def\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/core/tracing/decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/_telemetry/activity.py:372\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameter_dimensions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[0;32m--> 372\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[1;32m    375\u001b[0m         activityLogger\u001b[38;5;241m.\u001b[39mactivity_info\u001b[38;5;241m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:666\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_arm_id_or_upload_dependencies(job)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationException, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m ex:  \u001b[38;5;66;03m# pylint: disable=W0718\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mlog_and_raise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m git_props \u001b[38;5;241m=\u001b[39m get_git_properties()\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Do not add git props if they already exist in job properties.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# This is for update specifically-- if the user switches branches and tries to update\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# their job, the request will fail since the git props will be repopulated.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# MFE does not allow existing properties to be updated, only for new props to be added\u001b[39;00m\n",
      "File \u001b[0;32m~/class/dsan6000/working-repos/azure-project-tutorial/.venv/lib/python3.12/site-packages/azure/ai/ml/_exception_helper.py:337\u001b[0m, in \u001b[0;36mlog_and_raise_error\u001b[0;34m(error, debug, yaml_operation)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MlException(message\u001b[38;5;241m=\u001b[39mformatted_error, no_personal_data_message\u001b[38;5;241m=\u001b[39mformatted_error)\n",
      "\u001b[0;31mMlException\u001b[0m: \n\u001b[37m\n\u001b[30m\n1) One or more fields are invalid\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Could not parse azureml://sparknlp-env@latest. If providing an ARM id, it should start with a '/'.\u001b[39m\n\nResolutions: \n1) Double-check that all specified parameters are of the correct types and formats prescribed by the ArmResource schema.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://docs.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "sparknlp_job = ml_client.jobs.create_or_update(nlp_job_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1731375892859
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading project-tutorials (0.51 MBs): 100%|██████████| 514798/514798 [00:00<00:00, 1398860.37it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job2_object = azureml_client.jobs.create_or_update(job2_submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Job Studio URL\n",
    "\n",
    "Once you submit the job, you can navigate to it in the AzureML Studio and monitor it's progress. There are ways to do it through the SDK but for now just use the Studio. These are unattended jobs, which means you can shut down this notebook and the Compute Instance, but the job will go through it's lifecycle:\n",
    "\n",
    "- Job is submitted\n",
    "- Job is queued\n",
    "- Job is run\n",
    "- Job completes (assuming no errors)\n",
    "\n",
    "**Each job's Studio URL will be different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1731375905109
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml.azure.com/runs/cyan_lunch_frrhmxy3j0?wsid=/subscriptions/21ff0fc0-dd2c-450d-93b7-96eeb3699b22/resourcegroups/prof-azureml/workspaces/prof-azureml&tid=fd571193-38cb-437b-bb55-60f28d67b643\n"
     ]
    }
   ],
   "source": [
    "job1_url = job1_object.studio_url\n",
    "print(job1_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1731360053055
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml.azure.com/runs/cool_apple_77y7w93jt5?wsid=/subscriptions/21ff0fc0-dd2c-450d-93b7-96eeb3699b22/resourcegroups/prof-azureml/workspaces/prof-azureml&tid=fd571193-38cb-437b-bb55-60f28d67b643\n"
     ]
    }
   ],
   "source": [
    "job2_url = job1_object.studio_url\n",
    "print(job2_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
